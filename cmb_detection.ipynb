{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Modules"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# General ------------------------------------------------------------------------------------------------------------------\nimport os\nimport math\nimport random\nimport itertools\nimport scipy.io\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Image Augmentation -------------------------------------------------------------------------------------------------------\nimport imgaug as ia\nimport imgaug.augmenters as iaa\n\n# Deep Learning ------------------------------------------------------------------------------------------------------------\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers, initializers, layers, activations, metrics\n\n# Bayesian Optimization ----------------------------------------------------------------------------------------------------\nfrom bayes_opt import BayesianOptimization\nfrom bayes_opt import UtilityFunction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set random seeds ---------------------------------------------------------------------------------------------------------\nrSeed = 0\nrandom.seed(rSeed)\nia.seed(rSeed)\nnp.random.seed(rSeed)\n\n# Set the split fraction between train and test sets -----------------------------------------------------------------------\nsplitFraction = 0.7\n\n# Grid search settings -----------------------------------------------------------------------------------------------------\ngridSearchIterationLimit = float(\"inf\")                 # Set to infinity for searching the whole grid\n# gridSearchIterationLimit = 10\n\n# Random search settings ---------------------------------------------------------------------------------------------------\nrandomSearchIterationLimit = 847                        # Set to 847 for a fair comparison between random search and grid search\n# randomSearchIterationLimit = 10\n\n# Bayesian optimization settings -------------------------------------------------------------------------------------------\nbayeOptIterationLimit = 25\n# bayeOptIterationLimit = 10\n\n# Deep learning model settings ---------------------------------------------------------------------------------------------\nregEpochs = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Regression Model ---------------------------------------------------------------------------------------------------------------------------------\ndef createRegModel(lr=-1, beta1=0.9, beta2=0.999, epsilon=None, decay=0.0):\n    try: del model\n    except:pass\n    \n    # Create a sequential model --------------------------------------------------------------------------------------------\n    model = tf.keras.models.Sequential()\n    # Add a convolutional layer --------------------------------------------------------------------------------------------\n    model.add(layers.Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='valid', input_shape=(41, 41, 1)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n    model.add(layers.MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='valid'))\n    # Add another convolution layer ----------------------------------------------------------------------------------------\n    model.add(layers.Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n    model.add(layers.MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='valid'))\n    # Flatten the output ---------------------------------------------------------------------------------------------------\n    model.add(layers.Flatten())\n    # Add dense layers -----------------------------------------------------------------------------------------------------\n#     model.add(layers.Dense(2))\n#     model.add(layers.Dense(2))\n    model.add(layers.Dense(2))\n    # Add a softmex layer --------------------------------------------------------------------------------------------------\n    model.add(layers.Softmax())\n    # Set ADAM as the training algorithm -----------------------------------------------------------------------------------\n    opt = optimizers.Adam(lr=10**lr, beta_1=beta1, beta_2=beta2, epsilon=epsilon, decay=decay)\n    model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[metrics.CategoricalAccuracy()])\n    \n    # Print summary of the model -------------------------------------------------------------------------------------------\n#     print(model.inputs, model.outputs, model.summary(), sep=\"\\n\\n\")\n    return model\n### --------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n\n### Image Augmentation -------------------------------------------------------------------------------------------------------------------------------\ndef augmentImages(origImages):\n    seqAug = iaa.Sequential([iaa.OneOf([iaa.AdditiveGaussianNoise(loc=0, scale=(0, 0.1*255)), iaa.GaussianBlur(sigma=(0, 1))]),\n                            iaa.Affine(rotate=(-25, 25))])\n    augImages = seqAug.augment_images(origImages)\n    return augImages\n### --------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n\n### Bayesian Optimization ----------------------------------------------------------------------------------------------------------------------------\ndef objectiveFunction(lr=-4, beta1=0.9, beta2=0.999):\n    # Create the model using specific hyperparameters\n    model = createRegModel(lr=lr, beta1=beta1, beta2=beta2)\n    # Train the model\n    model.fit_generator(generator=trainBatchIterator, steps_per_epoch=len(trainBatchIterator), epochs=regEpochs)\n    # Evaluate model\n    modelScore = model.evaluate(trainX, categoricalTrainY)\n    # Return accuracy on train set\n    return modelScore[1]\n### --------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n### Metrics ------------------------------------------------------------------------------------------------------------------------------------------\ndef evaluateModel(method, predTestY):\n    tp = tf.keras.metrics.TruePositives()\n    tn = tf.keras.metrics.TrueNegatives()\n    fp = tf.keras.metrics.FalsePositives()\n    fn = tf.keras.metrics.FalseNegatives()\n    _ = tp.update_state(testY, predTestY)\n    _ = tn.update_state(testY, predTestY)   \n    _ = fp.update_state(testY, predTestY)\n    _ = fn.update_state(testY, predTestY)    \n    \n    # True Positives -------------------------------------------------------------------------------------------------------\n    truePos = tp.result().numpy()\n    # True Negatives -------------------------------------------------------------------------------------------------------\n    trueNeg = tn.result().numpy()\n    # False Positives ------------------------------------------------------------------------------------------------------\n    falsePos = fp.result().numpy()\n    # False Negatives ------------------------------------------------------------------------------------------------------\n    falseNeg = fn.result().numpy()\n    \n    # Accuracy -------------------------------------------------------------------------------------------------------------\n    accuracy = (truePos + trueNeg)/(truePos + trueNeg + falsePos + falseNeg)\n    # Sensitivity ----------------------------------------------------------------------------------------------------------\n    sensitivity = (truePos)/(truePos + falseNeg)\n    # Specificity ----------------------------------------------------------------------------------------------------------\n    specificity = (trueNeg)/(trueNeg + falsePos)\n    # Precision ------------------------------------------------------------------------------------------------------------\n    precision = (truePos)/(truePos + falsePos)\n    return method, accuracy, sensitivity, specificity, precision\n### --------------------------------------------------------------------------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Train/Test Sets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load dataset -------------------------------------------------------------------------------------------------------------\ndataX = scipy.io.loadmat(\"Dataset_41_13031.mat\")[\"Input\"]\ndataY = scipy.io.loadmat(\"Dataset_41_13031.mat\")[\"Target\"]\n\ntrainX, trainY, testX, testY = [], [], [], []\n# Create train and test splits ---------------------------------------------------------------------------------------------\nfor i in range(dataX.shape[3]):\n    if i < math.ceil(dataX.shape[3] * splitFraction):\n        trainX.append(dataX[:, :, :, i])\n        trainY.append(dataY[:, i])\n    else:\n        testX.append(dataX[:, :, :, i])\n        testY.append(dataY[:, i])\ntrainX = np.asarray(trainX)\ntrainY = np.reshape(np.asarray(trainY), (-1,))\ntestX = np.asarray(testX)\ntestY = np.reshape(np.asarray(testY), (-1,))\n\n# Augment train set images -------------------------------------------------------------------------------------------------\ntrainX = np.concatenate((augmentImages(trainX), trainX))\ntrainY = np.concatenate((trainY, trainY))\n\n# Print shapes of train/test sets ------------------------------------------------------------------------------------------\nprint(\"Shape of train data:\", trainX.shape)\nprint(\"Shape of train labels:\", trainY.shape)\nprint(\"Shape of test data:\", testX.shape)\nprint(\"Shape of test label:\", testY.shape)\n\n\n# Convert binary labels into categorical labels ----------------------------------------------------------------------------\ncategoricalTrainY = np.asarray(tf.keras.utils.to_categorical(trainY, num_classes=2))\n\n# Create image iterator objects --------------------------------------------------------------------------------------------\nimgGen = tf.keras.preprocessing.image.ImageDataGenerator()\ntrainBatchIterator = imgGen.flow(x=trainX, y=categoricalTrainY, batch_size=64, shuffle=True, seed=rSeed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preview Samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"index = 18\nplt.imshow(trainX[index, :, :, 0])\nprint(\"This sample belongs to the class:\", categoricalTrainY[index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyper-parameter space to probe -------------------------------------------------------------------------------------------\nlrList = [-6, -5, -4, -3, -2, -1, 0]\nbetaList = [0.0001, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.9999]\n\n# Implement grid search ----------------------------------------------------------------------------------------------------\nbestParameters = 0\nmaxAccuracy = 0\nfor i, parameters in enumerate(itertools.product(lrList, betaList, betaList)):\n    if i == gridSearchIterationLimit: break\n    print(\"\\nIteration:\", i+1, \"||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\")\n    print(\"{'params': {'beta1': \", parameters[1], \", 'beta2': \", parameters[2], \", 'lr': \", parameters[0], \"}}\", sep=\"\")\n    # Create the model using specific hyperparameters\n    model = createRegModel(lr=parameters[0], beta1=parameters[1], beta2=parameters[2])\n    # Train the model\n    model.fit_generator(generator=trainBatchIterator, steps_per_epoch=len(trainBatchIterator), epochs=regEpochs)\n    # Evaluate model\n    modelScore = model.evaluate(trainX, categoricalTrainY, verbose=0)\n    # Keep a record of parameters that give most accurate performance on train set\n    if modelScore[1] > maxAccuracy:\n        bestParameters = (i, modelScore[1], parameters)\n        maxAccuracy = modelScore[1]\n\n# Store the parameters found using Grid Search -----------------------------------------------------------------------------\ngridSearchParameters = bestParameters[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the result ---------------------------------------------------------------------------------------------------------\nprint(\"\\n\")\nprint(\"{\", \"'iteration': \", bestParameters[0]+1, \", 'target': \", bestParameters[1], \", 'params': {'beta1': \", bestParameters[2][1], \", 'beta2': \", bestParameters[2][2], \", 'lr': \", bestParameters[2][0], \"}}\", sep=\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyper-parameter space to probe -------------------------------------------------------------------------------------------\nparameterSetList = [ (np.random.uniform(low=-6, high=0),\n                      np.random.uniform(low=0.0001, high=0.9999),\n                      np.random.uniform(low=0.0001, high=0.9999)) for i in range(randomSearchIterationLimit) ]\n\n# Implement random search --------------------------------------------------------------------------------------------------\nbestParameters = 0\nmaxAccuracy = 0\nfor i, parameters in enumerate(parameterSetList):\n    print(\"\\nIteration:\", i+1, \"||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\")\n    print(\"{'params': {'beta1': \", parameters[1], \", 'beta2': \", parameters[2], \", 'lr': \", parameters[0], \"}}\", sep=\"\")\n    # Create the model using specific hyperparameters\n    model = createRegModel(lr=parameters[0], beta1=parameters[1], beta2=parameters[2])\n    # Train the model\n    model.fit_generator(generator=trainBatchIterator, steps_per_epoch=len(trainBatchIterator), epochs=regEpochs)\n    # Evaluate model\n    modelScore = model.evaluate(trainX, categoricalTrainY, verbose=0)\n    # Keep a record of parameters that give most accurate performance on train set\n    if modelScore[1] > maxAccuracy:\n        bestParameters = (i, modelScore[1], parameters)\n        maxAccuracy = modelScore[1]\n\n# Store the parameters found using Grid Search -----------------------------------------------------------------------------\nrandomSearchParameters = bestParameters[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the result ---------------------------------------------------------------------------------------------------------\nprint(\"\\n\")\nprint(\"{\", \"'iteration': \", bestParameters[0]+1, \", 'target': \", bestParameters[1], \", 'params': {'beta1': \", bestParameters[2][1], \", 'beta2': \", bestParameters[2][2], \", 'lr': \", bestParameters[2][0], \"}}\", sep=\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bayesian Optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set region to probe ------------------------------------------------------------------------------------------------------\npbounds = {\"lr\": (-6, 0), \"beta1\": (0.0001, 0.9999), \"beta2\": (0.0001, 0.9999)}\n\n# Create the bayesian optimizer --------------------------------------------------------------------------------------------\noptimizer = BayesianOptimization(f=objectiveFunction, pbounds=pbounds, random_state=rSeed,)\nutility = UtilityFunction(kind=\"ei\", kappa=2.5, xi=0.0)\n\n# Maximize the accuracy ----------------------------------------------------------------------------------------------------\nfor _ in range(bayeOptIterationLimit):\n    nextPoint = optimizer.suggest(utility)\n    target = objectiveFunction(**nextPoint)\n    optimizer.register(params=nextPoint, target=target)\n    print(target, nextPoint, end=\"\\n\\n\")\n\n# Store the parameters found using Grid Search -----------------------------------------------------------------------------\nbayeOptParameters = (optimizer.max[\"params\"][\"lr\"], optimizer.max[\"params\"][\"beta1\"], optimizer.max[\"params\"][\"beta2\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the result ---------------------------------------------------------------------------------------------------------\nprint(\"\\n\")\nprint(optimizer.max)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid search --------------------------------------------------------------------------------------------------------------\nprint(\"Evaluating performance using grid search...\", end=\"\")\n# Create the model using specific hyperparameters\nmodel = createRegModel(lr=gridSearchParameters[0], beta1=gridSearchParameters[1], beta2=gridSearchParameters[2])\n# Train the model\nmodel.fit_generator(generator=trainBatchIterator, steps_per_epoch=len(trainBatchIterator), epochs=regEpochs, verbose=0)\n# Evaluate model\ngridSearchResults = evaluateModel(\"Grid Search\", np.argmax(model.predict(testX), axis = 1))\nprint(\" done!\")\n\n\n# Random search ------------------------------------------------------------------------------------------------------------\nprint(\"Evaluating performance using random search...\", end=\"\")\n# Create the model using specific hyperparameters\nmodel = createRegModel(lr=randomSearchParameters[0], beta1=randomSearchParameters[1], beta2=randomSearchParameters[2])\n# Train the model\nmodel.fit_generator(generator=trainBatchIterator, steps_per_epoch=len(trainBatchIterator), epochs=regEpochs, verbose=0)\n# Evaluate model\nrandomSearchResults = evaluateModel(\"Random Search\", np.argmax(model.predict(testX), axis = 1))\nprint(\" done!\")\n\n\n# Bayesian optimization ----------------------------------------------------------------------------------------------------\nprint(\"Evaluating performance using Bayesian optimization...\", end=\"\")\n# Create the model using specific hyperparameters\nmodel = createRegModel(lr=bayeOptParameters[0], beta1=bayeOptParameters[1], beta2=bayeOptParameters[2])\n# Train the model\nmodel.fit_generator(generator=trainBatchIterator, steps_per_epoch=len(trainBatchIterator), epochs=regEpochs, verbose=0)\n# Evaluate model\nbayeOptResults = evaluateModel(\"Bayesian Optimization\", np.argmax(model.predict(testX), axis = 1))\nprint(\" done!\\n\")\n\n\n# Compile results ----------------------------------------------------------------------------------------------------------\nresultsDF = pd.DataFrame([gridSearchResults, randomSearchResults, bayeOptResults], columns = [\"Method\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"Precision\"])\nprint(resultsDF)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}